diff --git a/docs/sdy_dialect.md b/docs/sdy_dialect.md
index 674f8f5..639c927 100755
--- a/docs/sdy_dialect.md
+++ b/docs/sdy_dialect.md
@@ -1,9 +1,8 @@
 <!-- Autogenerated by mlir-tblgen; don't manually edit -->
-
 # 'sdy' Dialect
 
-_The Shardy (SDY) dialect defines an axis-based tensor sharding
-    representation and additional API components to attach shardings to tensors._
+The Shardy (SDY) dialect defines an axis-based tensor sharding
+representation and additional API components to attach shardings to tensors.
 
 [TOC]
 
@@ -13,6 +12,7 @@ _The Shardy (SDY) dialect defines an axis-based tensor sharding
 
 _Performs an all-gather communication along axes_
 
+
 Syntax:
 
 ```
@@ -60,20 +60,20 @@ Interfaces: `InferTypeOpInterface`, `Sdy_CollectiveOpInterface`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `tensor` | tensor of any type values |
+| `tensor` | tensor of any type values
 
 #### Results:
 
 | Result | Description |
 | :----: | ----------- |
-| `result` | tensor of any type values |
-
+| `result` | tensor of any type values
 
 
 ### `sdy.all_reduce` (sdy::AllReduceOp)
 
 _Perform an all-reduce comunication along axes_
 
+
 Syntax:
 
 ```
@@ -105,20 +105,20 @@ Interfaces: `InferTypeOpInterface`, `Sdy_CollectiveOpInterface`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `tensor` | tensor of any type values |
+| `tensor` | tensor of any type values
 
 #### Results:
 
 | Result | Description |
 | :----: | ----------- |
-| `result` | tensor of any type values |
-
+| `result` | tensor of any type values
 
 
 ### `sdy.all_slice` (sdy::AllSliceOp)
 
 _Performs a dynamic-slice operation along axes_
 
+
 Syntax:
 
 ```
@@ -167,20 +167,20 @@ Interfaces: `InferTypeOpInterface`, `Sdy_CollectiveOpInterface`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `tensor` | tensor of any type values |
+| `tensor` | tensor of any type values
 
 #### Results:
 
 | Result | Description |
 | :----: | ----------- |
-| `result` | tensor of any type values |
-
+| `result` | tensor of any type values
 
 
 ### `sdy.all_to_all` (sdy::AllToAllOp)
 
 _Performs an all-to-all communication along axes_
 
+
 Syntax:
 
 ```
@@ -236,20 +236,20 @@ Interfaces: `InferTypeOpInterface`, `Sdy_CollectiveOpInterface`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `tensor` | tensor of any type values |
+| `tensor` | tensor of any type values
 
 #### Results:
 
 | Result | Description |
 | :----: | ----------- |
-| `result` | tensor of any type values |
-
+| `result` | tensor of any type values
 
 
 ### `sdy.collective_permute` (sdy::CollectivePermuteOp)
 
 _Performs a collective-permute communication to replace axes_
 
+
 Syntax:
 
 ```
@@ -297,14 +297,13 @@ Interfaces: `InferTypeOpInterface`, `Sdy_CollectiveOpInterface`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `tensor` | tensor of any type values |
+| `tensor` | tensor of any type values
 
 #### Results:
 
 | Result | Description |
 | :----: | ----------- |
-| `result` | tensor of any type values |
-
+| `result` | tensor of any type values
 
 
 ### `sdy.constant` (sdy::ConstantOp)
@@ -344,14 +343,14 @@ Effects: `MemoryEffects::Effect{}`
 
 | Result | Description |
 | :----: | ----------- |
-| `output` | statically shaped tensor of any type values |
-
+| `output` | statically shaped tensor of any type values
 
 
 ### `sdy.data_flow_edge` (sdy::DataFlowEdgeOp)
 
 _Data flow edge op._
 
+
 Syntax:
 
 ```
@@ -426,20 +425,20 @@ Interfaces: `InferTypeOpInterface`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `input` | shaped of any type values |
+| `input` | shaped of any type values
 
 #### Results:
 
 | Result | Description |
 | :----: | ----------- |
-| `result` | shaped of any type values |
-
+| `result` | shaped of any type values
 
 
 ### `sdy.manual_computation` (sdy::ManualComputationOp)
 
 _Multi-device parallelism operation with manual collectives_
 
+
 Syntax:
 
 ```
@@ -485,20 +484,20 @@ Interfaces: `ShardableDataFlowOpInterface`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `tensors` | variadic of ranked tensor of any type values |
+| `tensors` | variadic of ranked tensor of any type values
 
 #### Results:
 
 | Result | Description |
 | :----: | ----------- |
-| `results` | variadic of ranked tensor of any type values |
-
+| `results` | variadic of ranked tensor of any type values
 
 
 ### `sdy.mesh` (sdy::MeshOp)
 
 _Named mesh_
 
+
 Syntax:
 
 ```
@@ -523,11 +522,11 @@ Interfaces: `Symbol`
 </table>
 
 
-
 ### `sdy.named_computation` (sdy::NamedComputationOp)
 
 _Named computation operation_
 
+
 Syntax:
 
 ```
@@ -575,20 +574,20 @@ Interfaces: `ConditionallySpeculatable`, `InferTypeOpInterface`, `ShardableDataF
 
 | Operand | Description |
 | :-----: | ----------- |
-| `operands` | variadic of any type |
+| `operands` | variadic of any type
 
 #### Results:
 
 | Result | Description |
 | :----: | ----------- |
-| &laquo;unnamed&raquo; | variadic of any type |
-
+&laquo;unnamed&raquo; | variadic of any type
 
 
 ### `sdy.propagation_barrier` (sdy::PropagationBarrierOp)
 
 _Propagation barrier operation_
 
+
 Syntax:
 
 ```
@@ -624,20 +623,20 @@ Effects: `MemoryEffects::Effect{}`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `input` | ranked tensor of any type values |
+| `input` | ranked tensor of any type values
 
 #### Results:
 
 | Result | Description |
 | :----: | ----------- |
-| `result` | ranked tensor of any type values |
-
+| `result` | ranked tensor of any type values
 
 
 ### `sdy.reshard` (sdy::ReshardOp)
 
 _Reshards a tensor to a different sharding_
 
+
 Syntax:
 
 ```
@@ -676,14 +675,13 @@ Effects: `MemoryEffects::Effect{}`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `input` | tensor of any type values |
+| `input` | tensor of any type values
 
 #### Results:
 
 | Result | Description |
 | :----: | ----------- |
-| `result` | tensor of any type values |
-
+| `result` | tensor of any type values
 
 
 ### `sdy.return` (sdy::ReturnOp)
@@ -694,12 +692,14 @@ _The `sdy.return` operation terminates the regions attached to
     of the same kind, e.g. `AnyTensor`) and therefore can be reused at various
     levels of the Shardy IR stack._
 
+
 Syntax:
 
 ```
 operation ::= `sdy.return` attr-dict ($results^ `:` type($results))?
 ```
 
+
 Traits: `AlwaysSpeculatableImplTrait`, `Terminator`
 
 Interfaces: `ConditionallySpeculatable`, `NoMemoryEffect (MemoryEffectOpInterface)`
@@ -710,14 +710,14 @@ Effects: `MemoryEffects::Effect{}`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `results` | variadic of any type |
-
+| `results` | variadic of any type
 
 
 ### `sdy.sharding_constraint` (sdy::ShardingConstraintOp)
 
 _Constrains a tensor to the specified sharding_
 
+
 Syntax:
 
 ```
@@ -754,20 +754,20 @@ Interfaces: `InferTypeOpInterface`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `input` | tensor of any type values |
+| `input` | tensor of any type values
 
 #### Results:
 
 | Result | Description |
 | :----: | ----------- |
-| `result` | tensor of any type values |
-
+| `result` | tensor of any type values
 
 
 ### `sdy.sharding_group` (sdy::ShardingGroupOp)
 
 _Constrains tensors in the group to have the same sharding._
 
+
 Syntax:
 
 ```
@@ -795,15 +795,14 @@ Interfaces: `InferTypeOpInterface`
 
 | Operand | Description |
 | :-----: | ----------- |
-| `input` | ranked tensor of any type values |
-
+| `input` | ranked tensor of any type values
 
 
 ## Attributes
 
 ### AxisRefAttr
 
-_Reference to either a full axis or a split sub-axis_
+Reference to either a full axis or a split sub-axis
 
 Syntax:
 
@@ -828,7 +827,7 @@ Syntax:
 
 ### AxisRefListAttr
 
-_List of axis refs_
+List of axis refs
 
 Syntax:
 
@@ -852,7 +851,7 @@ Syntax:
 
 ### DimMappingAttr
 
-_List of factor indices for a dimension_
+List of factor indices for a dimension
 
 An empty list indicates that this is a null mapping (this is parsed/printed
 with `*`), i.e. the dimension isn't mapped to any factors.
@@ -871,7 +870,7 @@ with `*`), i.e. the dimension isn't mapped to any factors.
 
 ### DimensionShardingAttr
 
-_Dimension sharding_
+Dimension sharding
 
 List of axis names to shard a tensor dimension on from major to minor, a
 boolean indicating whether the dimension can be further sharded, and an
@@ -896,7 +895,7 @@ highest priority is assumed when the priority is missing in the annotation.
 
 ### ListOfAxisRefListsAttr
 
-_List of axis ref lists_
+List of axis ref lists
 
 Syntax:
 
@@ -906,6 +905,8 @@ Syntax:
 >
 ```
 
+
+
 #### Parameters:
 
 | Parameter | C++ type | Description |
@@ -914,7 +915,7 @@ Syntax:
 
 ### ManualAxesAttr
 
-_A list of axes that a ManualComputationOp is manual on_
+A list of axes that a ManualComputationOp is manual on
 
 Syntax:
 
@@ -924,6 +925,8 @@ Syntax:
 >
 ```
 
+
+
 #### Parameters:
 
 | Parameter | C++ type | Description |
@@ -932,7 +935,7 @@ Syntax:
 
 ### MeshAttr
 
-_Mesh of axes and a list of devices_
+Mesh of axes and a list of devices
 
 Syntax:
 
@@ -986,7 +989,7 @@ Here are some examples of meshes:
 
 ### MeshAxisAttr
 
-_Named axis in a mesh_
+Named axis in a mesh
 
 Syntax:
 
@@ -997,6 +1000,8 @@ Syntax:
 >
 ```
 
+
+
 #### Parameters:
 
 | Parameter | C++ type | Description |
@@ -1006,7 +1011,7 @@ Syntax:
 
 ### OpShardingRuleAttr
 
-_Specifies how an operation can be partitioned._
+Specifies how an operation can be partitioned.
 
 Syntax:
 
@@ -1086,7 +1091,7 @@ for `stablehlo.custom_call` ops.
 
 ### SubAxisInfoAttr
 
-_Info about how this sub-axis is derived from the full axis_
+Info about how this sub-axis is derived from the full axis
 
 Syntax:
 
@@ -1121,7 +1126,7 @@ denoted as follows: `(m)k` for pre-size m and size k.
 
 ### TensorMappingAttr
 
-_Factor mappings for each dimension of a tensor._
+Factor mappings for each dimension of a tensor.
 
 Syntax:
 
@@ -1143,7 +1148,7 @@ Syntax:
 
 ### TensorShardingAttr
 
-_Tensor sharding_
+Tensor sharding
 
 Syntax:
 
@@ -1183,7 +1188,7 @@ name, referencing a corresponding `MeshOp` symbol, or an inlined `MeshAttr`.
 
 ### TensorShardingPerValueAttr
 
-_Tensor sharding per operand/result of an op_
+Tensor sharding per operand/result of an op
 
 Syntax:
 
@@ -1208,7 +1213,7 @@ A list of `TensorShardingAttr`s, one for each operand/result of an op.
 
 ### PropagationDirection
 
-_Propagation direction enum_
+propagation direction enum
 
 #### Cases:
 
@@ -1218,3 +1223,4 @@ _Propagation direction enum_
 | FORWARD | `1` | FORWARD |
 | BACKWARD | `2` | BACKWARD |
 | BOTH | `3` | BOTH |
+
diff --git a/docs/sdy_export_passes.md b/docs/sdy_export_passes.md
index 6404d4c..cc53465 100755
--- a/docs/sdy_export_passes.md
+++ b/docs/sdy_export_passes.md
@@ -1,13 +1,14 @@
 <!-- Autogenerated by mlir-tblgen; don't manually edit -->
-
 ### `-sdy-close-shardings`
 
 _Closes tensor shardings and drops replicated axes._
 
+
 ### `-sdy-drop-sharding-rules`
 
 _Drops `OpShardingRuleAttr` from all registered ops._
 
+
 ### `-sdy-insert-explicit-reshards`
 
 _Inserts explicit reshards to make all operations have compatible shardings._
@@ -57,11 +58,11 @@ are both sharded on axis "x" on their non-contracting dimensions. Here,
 `rhs` tensor is resharded before the dot operation explicitly, to be
 sharded only on its first dimension and on axis "x". This way, the dot
 operation becomes compatible.
-
 ### `-sdy-remove-sharding-groups`
 
 _Removes ShardingGroupOps after propagation._
 
+
 ### `-sdy-reshard-to-collectives`
 
 _Converts ReshardOp into various Shardy collective ops._
@@ -93,11 +94,11 @@ In the example above, the tensor `%0 : tensor<16x2xf32>` is sharded as
 after the reshard, we infer that we have all-gathered `{"y", "z"}`. The
 second dimension is not changed.
 
-
 ### `-sdy-sharding-constraint-to-reshard`
 
 _Converts ShardingConstraintOp into ReshardOp._
 
+
 ### `-sdy-sink-data-flow-edges`
 
 _Sinks all `DataFlowEdgeOp` into their input._
@@ -106,12 +107,10 @@ Moves the sharding of each `DataFlowEdgeOp` to its input (the root target of
 the edge), and replaces the op with its input.
 
 #### Options
-
 ```
 -sink-debug-sharding-origins          : Whether to sink the debug sharding origins info. See `debug-sharding-origins` option in propagation for more info.
 -sink-debug-propagation-edge-sharding : Whether to sink the debug propagation edge sharding info. See `debug-propagation-edge-sharding` option in propagation for more info.
 ```
-
 ### `-sdy-update-non-divisible-input-output-shardings`
 
 _Makes FuncOp inputs/outputs evenly sharded, removing any need for padding due to non-divisible shardings._
diff --git a/docs/sdy_import_passes.md b/docs/sdy_import_passes.md
index 8244641..8f55908 100755
--- a/docs/sdy_import_passes.md
+++ b/docs/sdy_import_passes.md
@@ -1,5 +1,4 @@
 <!-- Autogenerated by mlir-tblgen; don't manually edit -->
-
 ### `-sdy-add-data-flow-edges`
 
 _Inserts `DataFlowEdgeOp` for every data-flow edge._
@@ -10,7 +9,6 @@ the module.
 
 The inserted `DataFlowEdgeOp` will take the existing sharding of the owner
 target if it exists.
-
 ### `-sdy-apply-sharding-constraints`
 
 _Applies constraints that dictate the sharding of their input._
@@ -53,7 +51,6 @@ NOTE: The `in_shardings` of a `ManualComputationOp` are in essence sharding
 constraints on the corresponding operands, so this pass will also apply
 their sharding if the above conditions are satisfied (except for the
 dangling case).
-
 ### `-sdy-constant-splitter`
 
 _Splits constant sub-computations so each has a single use._
@@ -77,7 +74,6 @@ within that sub-computation.
 
 NOTE: This pass is the MLIR equivalent of `xla::HloConstantSplitter`,
 needed for the purpose of Shardy Propagation.
-
 ### `-sdy-lift-inlined-meshes`
 
 _Lifts inlined `MeshAttr`s in shardings as symbol `MeshOp`s._
@@ -92,7 +88,6 @@ The name of each new `MeshOp` will either be:
 * `maximal_mesh_{device-id}`, for a maximal mesh (i.e., empty axis list and
   a single device ID), or
 * The first available name in [`mesh`, `mesh_0`, `mesh_1`, ...].
-
 ### `-sdy-manual-axes-cleanup`
 
 _Cleans up the use of manual axes in `ManualComputationOp`s_
@@ -101,7 +96,6 @@ _Cleans up the use of manual axes in `ManualComputationOp`s_
    manual axis to its replicated_axes. This is to ensure manual axes are
    always fully specified.
 2. Sorts the manual axes in mesh axis declaration order.
-
 ### `-sdy-sharding-group-import`
 
 _Canonicalization and validation pass for sharding groups._
diff --git a/docs/sdy_op_interfaces.md b/docs/sdy_op_interfaces.md
index e0edb0d..c5d7800 100755
--- a/docs/sdy_op_interfaces.md
+++ b/docs/sdy_op_interfaces.md
@@ -1,7 +1,5 @@
 <!-- Autogenerated by mlir-tblgen; don't manually edit -->
-
 # OpInterface definitions
-
 ## CollectiveOpInterface (`Sdy_CollectiveOpInterface`)
 
 Interface for all collective ops. Encapsulates common get/set for
@@ -14,13 +12,11 @@ outSharding attribute.
 - Same rank for the operand and result sharding.
 
 ### Methods:
-
 #### `getOutSharding`
 
 ```c++
 ::mlir::sdy::TensorShardingAttr getOutSharding();
 ```
-
 Returns the output tensor sharding of the collective op.
 
 NOTE: This method *must* be implemented by the user.
@@ -30,7 +26,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 void setOutShardingAttr(::mlir::sdy::TensorShardingAttr sharding);
 ```
-
 Sets the output tensor sharding of the collective op.
 
 NOTE: This method *must* be implemented by the user.
@@ -40,7 +35,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 ::mlir::TypedValue<::mlir::TensorType> getTensor();
 ```
-
 Get the tensor operand of the collective op.
 
 NOTE: This method *must* be implemented by the user.
@@ -50,7 +44,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 ::mlir::Type getType();
 ```
-
 Get the type of the collective op result.
 
 NOTE: This method *must* be implemented by the user.
@@ -98,13 +91,11 @@ sources `x_i`, `return_value_i` and targets `y_i`, `pred_arg_i`,
 `body_arg_i`.
 
 ### Methods:
-
 #### `getBlockArgumentEdgeOwnerShardings`
 
 ```c++
 mlir::SmallVector<mlir::sdy::TensorShardingAttr> getBlockArgumentEdgeOwnerShardings();
 ```
-
 Returns the shardings of all block argument data flow edge owners.
 
 NOTE: This method *must* be implemented by the user.
@@ -114,7 +105,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 void setBlockArgumentEdgeOwnerShardings(mlir::ArrayRef<mlir::sdy::TensorShardingAttr> shardings);
 ```
-
 Sets `shardings` of all block argument edge owners.
 
 NOTE: This method *must* be implemented by the user.
@@ -124,7 +114,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 mlir::SmallVector<mlir::sdy::TensorShardingAttr> getOpResultEdgeOwnerShardings();
 ```
-
 Returns the shardings of all op result data flow edge owners.
 
 NOTE: This method *must* be implemented by the user.
@@ -134,7 +123,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 void setOpResultEdgeOwnerShardings(mlir::ArrayRef<mlir::sdy::TensorShardingAttr> shardings);
 ```
-
 Sets `shardings` of all op result edge owners.
 
 NOTE: This method *must* be implemented by the user.
@@ -144,7 +132,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 mlir::sdy::TensorShardingAttr transformTargetSharding(mlir::Value target, mlir::sdy::TensorShardingAttr sharding, mlir::sdy::DataFlowShardingTransformType transformType);
 ```
-
 Transforms the `sharding` of the target depending on `transformType`
 
 See `DataFlowShardingTransformType` for more information.
@@ -156,7 +143,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 mlir::ArrayRef<mlir::BlockArgument> getBlockArgumentEdgeOwners();
 ```
-
 Gets all block argument edge owners.
 
 NOTE: This method *must* be implemented by the user.
@@ -166,7 +152,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 mlir::ResultRange getOpResultEdgeOwners();
 ```
-
 Gets all op result edge owners.
 
 NOTE: This method *must* be implemented by the user.
@@ -176,7 +161,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 mlir::SmallVector<mlir::OpOperand*> getEdgeSources(mlir::Value owner);
 ```
-
 Gets the data flow edge sources given the edge `owner`.
 
 NOTE: This method *must* be implemented by the user.
@@ -186,7 +170,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 mlir::Value getEdgeOwnerFromTarget(mlir::Value target);
 ```
-
 Gets the owner `target` of a data flow edge given a `target` that may or
 may not be the owner.
 
@@ -197,7 +180,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 mlir::Value getEdgeOwnerFromSource(mlir::OpOperand&source);
 ```
-
 Gets the owner target of a data flow edge given a `source`.
 
 NOTE: This method *must* be implemented by the user.
@@ -207,7 +189,6 @@ NOTE: This method *must* be implemented by the user.
 ```c++
 mlir::SmallVector<mlir::Value> getNonEdgeOwnerTargets(mlir::Value owner);
 ```
-
 Gets the non-owner targets of a data flow edge given the edge `owner`.
 
 NOTE: This method *must* be implemented by the user.
@@ -221,13 +202,12 @@ the shape of the results, etc. See `OpShardingRuleAttr` for more
 details.
 
 ### Methods:
-
 #### `getShardingRule`
 
 ```c++
 mlir::sdy::OpShardingRuleAttr getShardingRule();
 ```
-
 Returns the sharding rule of the op.
 
 NOTE: This method *must* be implemented by the user.
+
diff --git a/docs/sdy_propagation_passes.md b/docs/sdy_propagation_passes.md
index bbc0754..d14d829 100755
--- a/docs/sdy_propagation_passes.md
+++ b/docs/sdy_propagation_passes.md
@@ -1,5 +1,4 @@
 <!-- Autogenerated by mlir-tblgen; don't manually edit -->
-
 ### `-sdy-aggressive-propagate`
 
 _Runs the aggressive sharding propagation algorithm._
@@ -23,7 +22,6 @@ memory footprint at the cost of potential communication.
    of a sharding on the MLIR module. These are what operand/result introduced a
    sharding on some op result.
 - `-propagation-strategy`: which factor propagation strategy to use.
-
 ### `-sdy-basic-propagate`
 
 _Runs the basic sharding propagation algorithm._
@@ -45,7 +43,6 @@ axes that are compatible between all operands and results.
 - `-debug-edge-source-sharding`: whether to save information about the edge source
    of a sharding on the MLIR module. These are what operand/result introduced a
    sharding on some op result.
-
 ### `-sdy-op-priority-propagate`
 
 _Runs the op-priority propagation algorithm._
@@ -83,7 +80,6 @@ applied (see `AggressivePropagationPass`).
 - `-propagation-strategy`: which factor propagation strategy to use.
 - `-run-op-priority-propagation`: whether to run (or skip) op-priority
    propagation.
-
 ### `-sdy-populate-op-sharding-rules`
 
 _Populates all registered ops with an `OpShardingRuleAttr`._
@@ -93,11 +89,9 @@ debugging/testing the registered sharding rules. Propagation already does
 this just-in-time, but this pass does it all at once.
 
 #### Options
-
 ```
 -conservative-propagation : whether to disllow rules that can propagate non-divisible sharding axes
 ```
-
 ### `-sdy-user-priority-propagate`
 
 _Runs the user-priority propagation algorithm._
diff --git a/third_party/llvm/workspace.bzl b/third_party/llvm/workspace.bzl
index 1deec6e..81a3b1c 100644
--- a/third_party/llvm/workspace.bzl
+++ b/third_party/llvm/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive")
 
 def repo(name):
     """Imports LLVM."""
-    LLVM_COMMIT = "0e779ad4998ef65907502101c5b82ede05ddfa4e"
-    LLVM_SHA256 = "d5c2560b2d9ce3ced7951113f2b5d1ea428665678f4dcb1fb8780eb1219ca615"
+    LLVM_COMMIT = "165a3d6a9b164dc98a70596fa8117acf3de20254"
+    LLVM_SHA256 = "ed072b520c6ff0e549fd59783edd4777bb79cd2b00dd44f15b2a4c1d960eec03"
 
     tf_http_archive(
         name = name,
diff --git a/third_party/stablehlo/temporary.patch b/third_party/stablehlo/temporary.patch
index 7acb5a7..b001f25 100755
--- a/third_party/stablehlo/temporary.patch
+++ b/third_party/stablehlo/temporary.patch
@@ -1,68 +1,3 @@
-diff --ruN a/stablehlo/BUILD.bazel b/stablehlo/BUILD.bazel
---- stablehlo/BUILD.bazel
-+++ stablehlo/BUILD.bazel
-@@ -519,8 +519,6 @@
-         "stablehlo/conversions/linalg/transforms/StablehloToLinalgRandom.cpp",
-         "stablehlo/conversions/linalg/transforms/StablehloToLinalgReduce.cpp",
-         "stablehlo/conversions/linalg/transforms/TypeConversion.cpp",
--        "stablehlo/transforms/conversions/TypeConversion.cpp",
--        "stablehlo/transforms/conversions/TypeConversion.h",
-     ],
-     hdrs = [
-         "stablehlo/conversions/linalg/transforms/LegalizeToLinalgUtils.h",
-@@ -531,6 +529,7 @@
-     ],
-     strip_include_prefix = ".",
-     deps = [
-+        "stablehlo_type_conversions",
-         ":base",
-         ":chlo_ops",
-         ":linalg_pass_inc_gen",
-@@ -943,6 +942,35 @@
-     ],
- )
- 
-+# Header-only target, used when using the C API from a separate shared library.
-+cc_library(
-+    name = "stablehlo_dialect_capi_headers",
-+    hdrs = STABLEHLO_DIALECT_CAPI_HEADERS,
-+    strip_include_prefix = ".",
-+    deps = [
-+        "@llvm-project//mlir:CAPIIRHeaders",
-+    ],
-+)
-+
-+# Alwayslink target, used when exporting the C API from a shared library.
-+cc_library(
-+    name = "stablehlo_dialect_capi_objects",
-+    srcs = STABLEHLO_DIALECT_CAPI_SOURCES,
-+    hdrs = STABLEHLO_DIALECT_CAPI_HEADERS,
-+    strip_include_prefix = ".",
-+    deps = [
-+        ":stablehlo_ops",
-+        ":stablehlo_portable_api",
-+        ":stablehlo_serialization",
-+        ":version",
-+        "@llvm-project//llvm:Support",
-+        "@llvm-project//mlir:CAPIIRObjects",
-+        "@llvm-project//mlir:IR",
-+        "@llvm-project//mlir:Support",
-+    ],
-+    alwayslink = True,
-+)
-+
- STABLEHLO_UNIFIED_CAPI_SOURCES = [
-     "stablehlo/integrations/c/StablehloPasses.cpp",
-     "stablehlo/integrations/c/StablehloUnifiedApi.cpp",
-@@ -1010,7 +1038,7 @@
-         ":linalg_passes",
-         ":reference_api",
-         ":reference_configuration",
--        ":stablehlo_dialect_capi",
-+        ":stablehlo_dialect_capi_objects",
-         ":stablehlo_ops",
-         ":stablehlo_passes",
-         ":stablehlo_portable_api",
 diff --ruN a/stablehlo/stablehlo/conversions/tosa/tests/binary.mlir b/stablehlo/stablehlo/conversions/tosa/tests/binary.mlir
 --- stablehlo/stablehlo/conversions/tosa/tests/binary.mlir
 +++ stablehlo/stablehlo/conversions/tosa/tests/binary.mlir
diff --git a/third_party/stablehlo/workspace.bzl b/third_party/stablehlo/workspace.bzl
index 248f23d..79480a0 100644
--- a/third_party/stablehlo/workspace.bzl
+++ b/third_party/stablehlo/workspace.bzl
@@ -4,8 +4,8 @@ load("//third_party:repo.bzl", "tf_http_archive", "tf_mirror_urls")
 
 def repo():
     #
-    STABLEHLO_COMMIT = "04c5a341abe43f50ea2f8eca97fcc07763230c00"
-    STABLEHLO_SHA256 = "3eb4968cd57544850bc3b2ea2f605501ef43e9f245aee0bf0c141d4748c2d277"
+    STABLEHLO_COMMIT = "7775e3e2f98fe458f227cdc41aec567535d1c796"
+    STABLEHLO_SHA256 = "a79617c9b234cc8758a6c36a85beed0991bd5d3f927bd34cc0c395e9178ab228"
     #
 
     tf_http_archive(
